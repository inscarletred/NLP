# NLP

Lamentablemente, no he podido sacar conclusiones del modelo debido a un error en el codigo. No sé si a lo mejor es cosa de google colab y a lor mejor al ejecutarlo en otro momento, si que va bien. Mi problema, como te habia comentado, es con el shape del iftdif matrix of X_train. Sin embargo, al intentar lo que me habias aconsejado con hacer un shape despues de cada transformacion, no he podido ver el error, pero parece que debidio a ese error, tampoco me sale el entrenamiento. Como verás en el codigo, he convertido a X_train en un array para asegurarme y el error del codigo de entrenamiento me sale que x_train no es un array, asi que me he quedado bloqueada. Me ha dado mucha pena dejarlo así porque lo estaba disfrutando y pensaba que iba a poder ver bien una prediccion y asi poder jugar con las metricas para mejorarlo. Al no poder sacar conclusiones del modelo, dejo a continuación una explicación de algunas conclusiones que he sacado.

He cogido a los reviews de las revistas y me he quedado con las columnas necesarias, las cuales son 'reviewText' y 'overall'. En este caso 'overall' es el numero de estrellas de las reseñas. Para asegurarme de que las reviews las tengo balanceadas, he hecho una visualizacion del score y para asegurarme de que me haya quedado con 2000 ejemplos de cada "score". Despues he creado clases de 0 y 1 para crear "positive" y "negative" review classes y he añadido una columna al df llmada "overall_label" con esas clases. 

Despues, al sacar las palabras más comunes y hacer la visualizacion de aquello, se ve que hace falta quitar las stop words porque se ve que las palabras que salen no nos dicen nada sobre las reseñas. Tambien he usado las n-grams de palabras más frequentes para ver que los conjuntos de 2 y 3 palabras tampoco nos dan insights. Entonces, aqui es donde he quitado las stop words. He vuelto a sacar las palabras mas comunes y al hacer eso, pintar un word cloud de ellas.

Luego, he usado nltk para tokenize y lemmatize las palabras para poder interpretar el significado del texto analizando la secuencia de las palabras y para estandarizar el texto. Al ejecutar la lemmatization, extraemos la forma basica de las palabras y asi nos ayuda a poder sacar del texto las palabras que estan haciendo "ruido", como en el caso de mi texto cuando aparecio una palabra, dentro de las que el IF IDF score mas alto, que era "aaaa". Cuando me salio esa palabra, volvi a mirar la lemmatizacion y vi que no lo habia hecho bien.

Luego hacemos el split entre train y test y empezamos con feature extraction. En esta fase tambien he creado un modelo de word2Vec y he pintado las 5 palabras más comunes y sus 10 palabras más parecidas. Es cierto que aqui me ha costado dibujarlo y no me ha quedado igual que el ejemplo que hicimos en clase. 

Luego, al sacar el TF IDF score para las 10 palabras con el score más alto y las 10 palabras con el score más bajo, vemos cuanta importantia tiene cada palabra relativo al corpus. Aqui no sé si el hecho de que me hayan salido las palabras más comunes con el score más bajo es porque no aportan tanta relevancia como alguno puede pensar, a pesar de ser las más usadas, o si es porque he hecho algo mal con el pre processing.

Al empezar con el entrenamiento es donde se ve que he tenido problemas. 
